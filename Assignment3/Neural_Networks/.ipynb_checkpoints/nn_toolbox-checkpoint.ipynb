{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# Scikit-learn Multi-layer Perceptron\n",
    "**Created by**: Jordi Smit & Rembrandt Klazinga\n",
    "\n",
    "This notebook uses the scikit-learn library for Python to train a neural network that is similar to the one you just built.\n",
    "It is up to you to run the code blocks below and fill in the hyperparameters where necessary, these are exlained in more detail later.\n",
    "Once all hyperparameters are sufficiently tweaked, you can compare the network's performance to that of your own.\n",
    "\n",
    "This notebook requires the following libraries:\n",
    "\n",
    " - numpy\n",
    " - matplotlib\n",
    " - scikit-learn\n",
    "\n",
    "The following code block will check if you have these libraries. If they are not installed this code block will automatically install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\liudas\\anaconda3\\lib\\site-packages (1.16.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\liudas\\anaconda3\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\liudas\\anaconda3\\lib\\site-packages (0.21.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\liudas\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\liudas\\anaconda3\\lib\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\liudas\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\liudas\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\liudas\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\liudas\\anaconda3\\lib\\site-packages (from scikit-learn) (0.13.2)\n",
      "Requirement already satisfied: six in c:\\users\\liudas\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\liudas\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (41.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "We start by importing all the modules we need. If this does not work make sure you have installed all the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (C:\\Users\\Liudas\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-044564d1d94a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (C:\\Users\\Liudas\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Next, we read in the data and split it into a train, validation and test set. Feel free to change the `test_fraction` and `validation_fraction` such that they match the fractions you used to train your own Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_seed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-dbe98560e0db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32massert\u001b[0m \u001b[1;36m0.0\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtraining_fraction\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"The training_fraction must be in the range (0.0, 1.0) but is {training_fraction}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_fraction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'random_seed' is not defined"
     ]
    }
   ],
   "source": [
    "features = np.genfromtxt(\"features.txt\", delimiter=\",\")\n",
    "targets = np.genfromtxt(\"targets.txt\", delimiter=\",\")\n",
    "labels = np.unique(targets)\n",
    "\n",
    "test_fraction = 0.15 # Feel free to change\n",
    "validation_fraction = 0.15 # Feel free to change\n",
    "training_fraction = 1 - test_fraction - validation_fraction\n",
    "\n",
    "assert 0.0 < test_fraction < 1.0, f\"The test_fraction must be in the range (0.0, 1.0) but is {test_fraction}\"\n",
    "assert 0.0 < validation_fraction < 1.0, f\"The validation_fraction must be in the range (0.0, 1.0) but is {validation_fraction}\"\n",
    "assert 0.0 < training_fraction < 1.0, f\"The training_fraction must be in the range (0.0, 1.0) but is {training_fraction}\"\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, targets, test_size=test_fraction, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the optimal hyperparameters\n",
    "We are going to use grid search to find the optimal combination of hyperparameters. We have the following hyperparameters:\n",
    "\n",
    " - **hidden_layer_sizes**: A list of integers where by the ith element is the number of neurons in the ith hidden layer.\n",
    " - **activation**: Activation function for the hidden layer. You have the following options:\n",
    "\t - 'identity' $f(x) = x$\n",
    "\t - 'logistic' $f(x) = \\frac{1}{ 1 + e^{-x}}$\n",
    "\t - 'tanh'  $f(x) = tanh(x)$\n",
    "\t - 'relu' $f(x) = max(0, x)$\n",
    " - **alpha**: L2 penalty (regularization term) parameter.\n",
    " - **solver**: The solver for weight optimization. You have the following options:\n",
    "\t - 'sgd' The original version stochastic gradient descent.\n",
    "\t - ‘adam’ A specialized version that automaticaly adapts it learning rates over time. Adam is significantly faster then SGD but uses more memory space.\n",
    " - **learning_rate_init**: The initial learning rate used. It controls the step-size in updating the weights.\n",
    " - **max_iter**: Maximum number of epochs (how many times each data point will be used, not the number of gradient steps). The solver iterates until convergence (determined by ‘tol’) or this number of iterations.\n",
    " - **batch_size**:  The size of minibatches that will be given to stochastic optimizer. When set to 'auto', `batch_size=min(200,  n_samples)`\n",
    " - **validation_fraction**: The proportion of training data to set aside as validation set for early stopping. This hyperparameters is calculated automitcaly by our notebook, so you should not change this.\n",
    " - **early_stopping**: If set to true, it will terminate the training process when validation score is not improving by at least tol for `n_iter_no_change` consecutive epochs.\n",
    " - **n_iter_no_change**: Maximum number of epochs to not meet `tol` improvement.\n",
    " - **tol**: Tolerance for the optimization. When the loss or score is not improving by at least `tol` for `n_iter_no_change` consecutive iterations.\n",
    "\n",
    "In the code below we have setup up grid search using [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) for Sklearn's [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html). We have provided you with some default values for each of the hyperparameters. Feel free to try out additional values. You can probably improve the network by finding better values for `hidden_layer_sizes`, `activation`, `alpha` and `learning_rate_init`. You can do this by adding additional values to the list attributes of `grid`. The code below will provide you with the best values in the grid you have defined.\n",
    "\n",
    "\n",
    "Be aware that the computational cost increases combinatorially with the number of options. Which means that you should be either very patient or you should choose wisely which values you want to try out. The grid search runs multiple training processes in parallel, making it a bit faster. By default this notebook will use all the processes it can get. However, this might be too much for some computers. If this is the case for you change `n_jobs` from `-1` to the number of processes you think is right (at least 1 of course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search on grid:\n",
      "-hidden_layer_sizes: [[5, 5], [5, 5, 5]]\n",
      "-activation: ['logistic']\n",
      "-alpha: [0, 0.01]\n",
      "-solver: ['adam']\n",
      "-learning_rate_init: [0.005, 0.01]\n",
      "-max_iter: [200]\n",
      "-batch_size: ['auto']\n",
      "-validation_fraction: [0.12749999999999997]\n",
      "-early_stopping: [True]\n",
      "-n_iter_no_change: [10]\n",
      "-tol: [0.0001]\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-85163f17e042>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# Performing grid search to find best parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_network\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "n_jobs = -1\n",
    "\n",
    "grid = [{\n",
    "    # Network architecture\n",
    "    'hidden_layer_sizes': [[5,5], [5, 5, 5]], # A list of integers\n",
    "    'activation': [\"logistic\"], # \"identity\", \"logistic\", \"tanh\", \"relu\"\n",
    "    'alpha': [0, 0.01], # Float\n",
    "    \n",
    "    # Optimizations parameters\n",
    "    'solver': [\"adam\"], # \"sgd\", \"adam\"\n",
    "    'learning_rate_init': [0.005, 0.01], # Float\n",
    "    \n",
    "    # Fitting parameters\n",
    "    'max_iter': [200], # Integer\n",
    "    'batch_size': [\"auto\"], # \"auto\" or an integer.\n",
    "    \n",
    "    # Validation\n",
    "    'validation_fraction': [validation_fraction * len(features) * (1 - test_fraction) / len(features)], #Do not change this one.\n",
    "    'early_stopping': [True], # Boolean, it is best to leave this on True.\n",
    "    'n_iter_no_change': [10],# Integer\n",
    "    'tol': [1e-4], # Float\n",
    "}]\n",
    "\n",
    "\n",
    "grid_network = MLPClassifier(verbose=False)\n",
    "\n",
    "\n",
    "print(f\"Grid search on grid:\")\n",
    "for k in grid[0]:\n",
    "    print(f\"-{k}: {grid[0][k]}\")\n",
    "print()\n",
    "\n",
    "# Performing grid search to find best parameters\n",
    "grid_search = GridSearchCV(grid_network, grid, n_jobs=n_jobs, verbose=True)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Best parameters found by grid search:\")\n",
    "for k in grid_search.best_params_:\n",
    "    print(f\"-{k}: {grid_search.best_params_[k]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing the best network\n",
    "The grid search has provided us with the network with the best combination of hyperparameters (given our train, validation set and grid). In this section, we analyse this network in multiple aspects. Firstly, we compare the difference between the loss function (Sklearn uses [Multi Class Cross-Entropy](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#id11) loss), accuracy and MSE on the training and test set. These values should be relatively close else we are overfitting on the training data. Secondly, we look at the training loss and validation curve. These curves will give us some insight into how much epochs we need and if we need to stop earlier or not. Finally, we compare the confusion matrix of the training and test data set. This will give us some insight into which classes the network finds hard to predict and which classes generalize less well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the network based on grid search results\n",
    "network = grid_search.best_estimator_\n",
    "\n",
    "# Printing the Cross-Entropy losses\n",
    "print(f\"Resulting Training Cross-Entropy loss: {network.loss_curve_[-1]}\")\n",
    "print(f\"Resulting Test Cross-Entropy loss: {log_loss(y_test, network.predict_proba(x_test), labels=labels)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Printing the accuracies\n",
    "print(f\"Resulting Training accuracy: {network.score(x_train, y_train)}\")\n",
    "print(f\"Resulting Validation accuracy: {network.validation_scores_[-1]}\")\n",
    "print(f\"Resulting Test accuracy: {network.score(x_test, y_test)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Printing the MSEs\n",
    "mse_train = mean_squared_error(OneHotEncoder().fit_transform(y_train.reshape(-1, 1)).toarray(), network.predict_proba(x_train))\n",
    "mse_test = mean_squared_error(OneHotEncoder().fit_transform(y_test.reshape(-1, 1)).toarray(), network.predict_proba(x_test))\n",
    "print(f\"Resulting Training MSE: {mse_train}\")\n",
    "print(f\"Resulting Test MSE: {mse_test}\")\n",
    "\n",
    "# Plot Training loss curve\n",
    "plt.plot(network.loss_curve_)\n",
    "plt.grid()\n",
    "plt.title(\"Training loss curve\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training Cross-Entropy loss\")\n",
    "plt.show()\n",
    "\n",
    "# Plot Validation accuracy curve\n",
    "plt.plot(network.validation_scores_)\n",
    "plt.grid()\n",
    "plt.title(\"Validation accuracy curve\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation accuracy\")\n",
    "plt.show()\n",
    "\n",
    "# Plot training data Confusion Matrix\n",
    "figure = plt.figure(figsize=(8,8))\n",
    "ax = figure.add_subplot()\n",
    "disp = plot_confusion_matrix(network, x_train, y_train,\n",
    "                                 display_labels=labels,\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=\"true\",\n",
    "                                 ax=ax\n",
    "                            )\n",
    "disp.ax_.set_title(\"Training Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Plot test data Confusion Matrix\n",
    "figure = plt.figure(figsize=(8,8))\n",
    "ax = figure.add_subplot()\n",
    "disp = plot_confusion_matrix(network, x_test, y_test,\n",
    "                                 display_labels=labels,\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=\"true\",\n",
    "                                 ax=ax\n",
    "                            )\n",
    "disp.ax_.set_title(\"Confusion Matrix test data\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
